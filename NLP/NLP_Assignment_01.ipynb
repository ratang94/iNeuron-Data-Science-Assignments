{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_01.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Submitted By: Gaurav Ratan**"
      ],
      "metadata": {
        "id": "WvuEP0kLpV8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain One-Hot Encoding\n",
        "\n",
        "Ans: \n",
        "\n",
        "One hot encoding can be defined as the essential process of converting the categorical data variables to be provided to machine and deep learning algorithms which in turn improve predictions as well as classification accuracy of a model\n",
        "\n",
        "Categorical data refers to variables that are made up of label values, for example, a “color” variable could have the values “red“, “blue, and “green”. Think of values like different categories that sometimes have a natural ordering to them.\n",
        "\n",
        "Some machine learning algorithms can work directly with categorical data depending on implementation, such as a decision tree, but most require any inputs or outputs variables to be a number, or numeric in value. This means that any categorical data must be mapped to integers.\n",
        "\n",
        "One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1"
      ],
      "metadata": {
        "id": "-lko5IojpYrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Explain Bag of Words\n",
        "\n",
        "Ans:\n",
        "\n",
        "A bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of word counts and disregard the grammatical details and the word order. It is called a “bag” of words because any information about the order or structure of words in the document is discarded. A bag-of-words is a representation of text that describes the occurrence of words within a document. \n",
        "\n",
        "It involves two things:\n",
        "A vocabulary of known words.\n",
        "A measure of the presence of known words.\n",
        "\n",
        "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
      ],
      "metadata": {
        "id": "UfvPugnWpk7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Explain Bag of N-Grams\n",
        "\n",
        "Ans:\n",
        "\n",
        "Bag of n-grams is a natural extension of bag of words. An n-gram is simply any sequence of n tokens (words). Consequently, given the following review text - “Absolutely wonderful - silky and sexy and comfortable”, we could break this up into: 1-grams: Absolutely, wonderful, silky, and, sexy, and, comfortable.\n",
        "\n",
        "let’s use the following phrase and divide it into bi-grams (n=2).\n",
        "\n",
        "James is the best person ever. becomes:\n",
        "\n",
        "  <start>James\n",
        "\n",
        "  James is\n",
        "\n",
        "  is the\n",
        "\n",
        "  the best\n",
        "\n",
        "  best person\n",
        "\n",
        "  person ever.\n",
        "  \n",
        "  ever.<end>\n",
        "\n",
        "In a typical bag-of-n-grams model, these 6 bigrams would be a sample from a large number of bigrams observed in a corpus. And then James is the best person ever. would be encoded in a representation showing which of the corpus’s bigrams were observed in the sentence.\n",
        "\n",
        "A bag-of-n-grams model has the simplicity of the bag-of-words model, but allows the preservation of more word locality information."
      ],
      "metadata": {
        "id": "iPdzOnT8pk5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Explain TF-IDF\n",
        "\n",
        "Ans:\n",
        "\n",
        "TF-IDF which means Term Frequency and Inverse Document Frequency, is a scoring measure widely used in information retrieval (IR) or summarization. TF-IDF is intended to reflect how relevant a term is in a given document."
      ],
      "metadata": {
        "id": "6V1aotumpk3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is OOV problem?\n",
        "\n",
        "Ans:\n",
        "\n",
        "Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. In speech recognition, it's the audio signal that contains these terms. Word vectors are the mathematical equivalent of word meaning."
      ],
      "metadata": {
        "id": "HOBKJw9Iqlzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What are word embeddings?\n",
        "\n",
        "Ans:\n",
        "\n",
        "It is an approach for representing words and documents. Word Embedding or Word Vector is a numeric vector input that represents a word in a lower-dimensional space. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features.\n",
        "\n",
        "Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector has values corresponding to these features.\n",
        "\n",
        "Goal of Word Embeddings\n",
        "\n",
        "To reduce dimensionality\n",
        "\n",
        "To use a word to predict the words around it\n",
        "\n",
        "Inter word semantics must be captured\n",
        "\n",
        "They are used as input to machine learning models.\n",
        "\n",
        "Take the words —-> Give their numeric representation —-> Use in training or inference\n",
        "\n",
        "To represent or visualize any underlying patterns of usage in the corpus that was used to train them."
      ],
      "metadata": {
        "id": "iOguz6niqlxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Explain Continuous bag of words (CBOW)\n",
        "\n",
        "Ans:\n",
        "\n",
        "Continuous Bag of Words Model (CBOW): Both are architectures to learn the underlying word representations for each word by using neural networks. ... In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle .\n",
        "\n",
        "The CBOW model tries to understand the context of the words and takes this as input. It then tries to predict words that are contextually accurate. Let us consider an example for understanding this. Consider the sentence: ‘It is a pleasant day’ and the word ‘pleasant’ goes as input to the neural network. We are trying to predict the word ‘day’ here. We will use the one-hot encoding for the input words and measure the error rates with the one-hot encoded target word. Doing this will help us predict the output based on the word with least error."
      ],
      "metadata": {
        "id": "axJVSKOcqlu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Explain SkipGram\n",
        "\n",
        "Ans:\n",
        "\n",
        "Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It’s reverse of CBOW algorithm. Here, target word is input while context words are output.\n",
        "\n",
        "The main idea behind the Skip-Gram model is this: it takes every word in a large corpora (we will call it the focus word) and also takes one-by-one the words that surround it within a defined 'window' to then feed a neural network that after training will predict the probability for each word to actually appear in the window around the focus word."
      ],
      "metadata": {
        "id": "FELPyLOQrAEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Explain Glove Embeddings.\n",
        "\n",
        "Ans:\n",
        "\n",
        "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space."
      ],
      "metadata": {
        "id": "VhDyhP0IrAA4"
      }
    }
  ]
}