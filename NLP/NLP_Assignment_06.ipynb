{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_06.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Submitted By: Gaurav Ratan**"
      ],
      "metadata": {
        "id": "WvuEP0kLpV8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What are Vanilla autoencoders\n",
        "\n",
        "Ans: \n",
        "\n",
        "The vanilla autoencoder, as proposed by Hinton, consists of only one hidden layer. The number of neurons in the hidden layer is less than the number of neurons in the input (or output) layer. This results in producing a bottleneck effect on the flow of information in the network, and therefore we can think of the hidden layer as a bottleneck layer, restricting the information that would be stored.\n",
        "\n",
        "![0_V0GyOt3LoDVfY7y5.png](https://www.researchgate.net/profile/Guillaume-Alexandre-Bilodeau/publication/327434418/figure/fig2/AS:667345727025158@1536119028351/Diagram-showing-a-Vanilla-Autoencoder.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-lko5IojpYrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.  What are Sparse autoencoders\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ans:\n",
        "\n",
        "A Sparse Autoencoder is a type of autoencoder that employs sparsity to achieve an information bottleneck. Specifically the loss function is constructed so that activations are penalized within a layer.\n",
        "\n",
        "![0_V0GyOt3LoDVfY7y5.png](https://www.researchgate.net/profile/Mohamed-Loey/publication/317734695/figure/fig1/AS:507899973730304@1498104198885/Sparse-autoencoder-structure.png)\n"
      ],
      "metadata": {
        "id": "UfvPugnWpk7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.  What are Denoisingautoencoders?\n",
        "\n",
        "Ans:\n",
        "\n",
        "Denoising autoencoders are an extension of the basic autoencoder, and represent a stochastic version of it. Denoising autoencoders attempt to address identity-function risk by randomly corrupting input (i.e. introducing noise) that the autoencoder must then reconstruct, or denoise.\n",
        "\n",
        "\n",
        "![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/max/1400/0*ECdHu2yeal38Jl3P.png)\n"
      ],
      "metadata": {
        "id": "iPdzOnT8pk5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are Convolutional autoencoders\n",
        "\n",
        "Ans:\n",
        "\n",
        "Convolutional Autoencoder is a variant of Convolutional Neural Networks that are used as the tools for unsupervised learning of convolution filters. They are generally applied in the task of image reconstruction to minimize reconstruction errors by learning the optimal filters.\n",
        "\n",
        "![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/max/1400/1*KxeovF6fhyN12cH4vbpOEA.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fVP_o59x6Cmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What are Stacked autoencoders\n",
        "\n",
        "Ans:\n",
        "\n",
        "A stacked autoencoder is a neural network consist several layers of sparse autoencoders where output of each hidden layer is connected to the input of the successive hidden layer. ... The learned data from the previous layer is used as an input for the next layer and this continues until the training is completed.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6V1aotumpk3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain how to generate sentences using LSTM autoencoders\n",
        "Ans:\n",
        "\n",
        "Creating an LSTM Autoencoder in Keras can be achieved by implementing an Encoder-Decoder LSTM architecture and configuring the model to recreate the input sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "x3bo17oM6RGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Explain Extractive summarization\n",
        "\n",
        "Ans:\n",
        "\n",
        "Extractive summarization aims at identifying the salient information that is then extracted and grouped together to form a concise summary. Abstractive summary generation rewrites the entire document by building internal semantic representation, and then a summary is created using natural language processing."
      ],
      "metadata": {
        "id": "c-ylSkrghF_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Explain Abstractive summarization\n",
        "\n",
        "Ans:\n",
        "\n",
        "Abstractive summarization, on the other hand is a technique in which the summary is generated by generating novel sentences by either rephrasing or using the new words, instead of simply extracting the important sentences.\n"
      ],
      "metadata": {
        "id": "rHoq_IdwhF8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Explain Beam search\n",
        "\n",
        "Ans:\n",
        "\n",
        "Beam search is a heuristic search algorithm that explores a graph by expanding the most optimistic node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements.\n",
        "\n",
        "Best-first search is a graph search that orders all partial solutions according to some heuristic. But in beam search, only a predetermined number of best partial solutions are kept as candidates. Therefore, it is a greedy algorithm.\n",
        "\n",
        "Beam search uses breadth-first search to build its search tree. At each level of the tree, it generates all successors of the states at the current level, sorting them in increasing order of heuristic cost. However, it only stores a predetermined number (Î²), of best states at each level called the beamwidth. Only those states are expanded next.\n",
        "\n",
        "![0_V0GyOt3LoDVfY7y5.png](https://i.stack.imgur.com/1A9Ed.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "Gdc99k7MhF59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Explain Length normalization\n",
        "\n",
        "Ans:\n",
        "\n",
        "Document length normalization adjusts the term frequency or the relevance score in order to normalize the effect of document length on the document ranking.\n"
      ],
      "metadata": {
        "id": "ATb7INT8hF3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.\tExplain Coverage normalization\n",
        "\n",
        "Ans:\n",
        "\n",
        "To calculate normalized coverage, the coverage is divided by the average coverage over all 10 base positions.The mean sequencing coverage required is calculated by dividing the desired coverage by the mean normalized coverage."
      ],
      "metadata": {
        "id": "fHg42KHIXl-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12.\tExplain ROUGE metric evaluation\n",
        "\n",
        "Ans:\n",
        "\n",
        "The Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scoring algorithm evaluates the similarity between a candidate document and a collection of reference documents. Use the ROUGE score to evaluate the quality of document translation and summarization models."
      ],
      "metadata": {
        "id": "8Ye6z24aXl6B"
      }
    }
  ]
}