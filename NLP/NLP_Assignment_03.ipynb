{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_03.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Submitted By: Gaurav Ratan**"
      ],
      "metadata": {
        "id": "WvuEP0kLpV8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the basic architecture of RNN cell.\n",
        "\n",
        "Ans: \n",
        "\n",
        "Recurrent Neural Network: The fundamental feature of a Recurrent Neural Network (RNN) is that the network contains at least one feed-back connection, so the activations can flow round in a loop. That enables the networks to do temporal processing and learn sequences, e.g., perform sequence recognition/reproduction or temporal association/prediction. Recurrent neural network architectures can have many different forms. One common type consists of a standard Multi-\n",
        "\n",
        "Layer Perceptron (MLP) plus added loops. These can exploit the powerful non-linear mapping capabilities of the MLP, and also have some form of memory. Others have more uniform structures, potentially with every neuron connected to all the others, and may also have stochastic activation functions. For simple architectures and deterministic activation functions, learning can be achieved using similar gradient descent procedures to those leading to the back-propagation algorithm for feed-forward networks.\n",
        "\n",
        "In sequential tasks such as natural language and speech processing, there is always dependence of present input data upon the previous applied inputs. Task of RNNs is to find the relationship between current input and the previous applied inputs. In theory RNNs can make use of information sequence of any arbitrarily length, but in practice they are limited to looking back only a few steps.\n",
        "\n",
        "\"\\\"![0_V0GyOt3LoDVfY7y5.png](https://www.researchgate.net/publication/332663947/figure/fig1/AS:751783865511938@1556250649554/Simple-RNN-cell-structure-in-hidden-layer-b.png)\\\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "-lko5IojpYrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Explain Backpropagation through time (BPTT)\n",
        "\n",
        "Ans:\n",
        "\n",
        "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series. A recurrent neural network is shown one input each timestep and predicts one output. Conceptually, BPTT works by unrolling all input timesteps.\n",
        "\n",
        "\n",
        "\"\\\"![0_V0GyOt3LoDVfY7y5.png](https://raw.githubusercontent.com/mmuratarat/mmuratarat.github.io/master/_posts/images/BPTT.png)\\\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "UfvPugnWpk7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Explain Vanishing and exploding gradients\n",
        "\n",
        "Ans:\n",
        "\n",
        "Two common problems that occur during the backpropagation of time-series data are the vanishing and exploding gradients. The equation above has two problematic cases:\n",
        "\n",
        "In the first case, the term goes to zero exponentially fast, which makes it difficult to learn some long period dependencies. This problem is called the vanishing gradient. In the second case, the term goes to infinity exponentially fast, and their value becomes a NaN due to the unstable process.\n",
        "\n",
        "This problem is called the exploding gradient. In the following two sections, we review two approaches to deal with these problems."
      ],
      "metadata": {
        "id": "iPdzOnT8pk5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Explain Long short-term memory (LSTM)\n",
        "\n",
        "Ans:\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\n",
        "\n",
        "This is a behavior required in complex problem domains like machine translation, speech recognition, and more.\n",
        "\n",
        "LSTMs are a complex area of deep learning. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional and sequence-to-sequence relate to the field.\n",
        "\n",
        "\"\\\"![0_V0GyOt3LoDVfY7y5.png](https://www.researchgate.net/profile/Gonzalo-Napoles/publication/340493274/figure/fig1/AS:877838903308288@1586304512343/Architecture-of-a-typical-vanilla-LSTM-block.png)\\\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "fVP_o59x6Cmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Explain Gated recurrent unit (GRU)\n",
        "\n",
        "Ans:\n",
        "\n",
        "To solve the vanishing gradient problem of a standard RNN, GRU uses, so-called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output. The special thing about them is that they can be trained to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction.\n",
        "\n",
        "\"\\\"![0_V0GyOt3LoDVfY7y5.png](https://blog.floydhub.com/content/images/2019/07/image14.jpg)\\\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6V1aotumpk3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain Peephole LSTM\n",
        "Ans:\n",
        "\n",
        "One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state. In this peephole connection we can see that all the gates are having an input along with the cell state.\n",
        "\n",
        "\"\\\"![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/max/1400/0*9ofTZTHKBGuMwTxd.png)\\\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "x3bo17oM6RGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Bidirectional RNNs\n",
        "\n",
        "Ans:\n",
        "\n",
        "Bidirectional recurrent neural networks (BRNN) connect two hidden layers of opposite directions to the same output. With this form of generative deep learning, the output layer can get information from past (backwards) and future (forward) states simultaneously.\n",
        "\n",
        "BRNNs were introduced to increase the amount of input information available to the network. For example, multilayer perceptron (MLPs) and time delay neural network (TDNNs) have limitations on the input data flexibility, as they require their input data to be fixed. Standard recurrent neural network\n",
        "\n",
        "(RNNs) also have restrictions as the future input information cannot be reached from the current state. On the contrary, BRNNs do not require their input data to be fixed. Moreover, their future input information is reachable from the current state.\n",
        "\n",
        "BRNN are especially useful when the context of the input is needed. For example, in handwriting recognition, the performance can be enhanced by knowledge of the letters located before and after the current letter."
      ],
      "metadata": {
        "id": "71a4xcTf6REX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Explain the gates of LSTM with equations.\n",
        "\n",
        "Ans:\n",
        "\n",
        "\"\\\"![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/max/1400/0*T_-9HypnVL09rnuo.jpeg)\\\"\"\n",
        "\n",
        "**Forget Gate**: This is the first step in LSTM network which decides which information would be passed threw the cell state and this decision is taken by forget gate. It takes ht-1 and xt and an input and output is given as 0 and 1 which is then point wise multiplied with Ct-1 and finally it will decide which information would be passes threw. If we are working with some context based data if the context is changed this forget cell will discard the information which is not relevant to context.\n",
        "\n",
        "**Input Gate**: In this step will decide which information is going to be stored in cell state. This operation is one in two step. First, a sigmoid neural network decides which values we will update and a tanh layer that creates a vector of new candidate values, Ct, that could be added to the state.\n",
        "\n",
        "**Output Gate**: At this stage, we have to decide about what we are going to send in output. This would be based on our cell state, but we run a sigmoid neural network which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we can decide about output data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h1TZjXWc6RCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Explain BiLSTM\n",
        "\n",
        "Ans:\n",
        "\n",
        "A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction.\n",
        "\n",
        "The CNN is used to extract the local features of the text vector, and the BiLSTM is used to extract the global features related to the text context. ... After preprocessing the corpus, the text is expressed as a two-dimensional word vector matrix, and then the local information features are extracted by using the CNN."
      ],
      "metadata": {
        "id": "aAn1HoO86Q_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Explain BiGRU\n",
        "Ans:\n",
        "\n",
        "A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates."
      ],
      "metadata": {
        "id": "2aqC8dXA6Q9M"
      }
    }
  ]
}